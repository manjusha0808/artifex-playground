# XPath Crawler - Repository Folder Structure

## Repository Layout

```
xpath-crawler/
‚îÇ
‚îú‚îÄ‚îÄ .artifex/                          # Documentation & specifications (this folder)
‚îÇ   ‚îú‚îÄ‚îÄ GUIDELINES.md                  # DO's and DON'Ts for development
‚îÇ   ‚îú‚îÄ‚îÄ SPECS.md                       # Technical specifications & architecture
‚îÇ   ‚îî‚îÄ‚îÄ FOLDER_STRUCTURE.md            # This file - folder organization guide
‚îÇ
‚îú‚îÄ‚îÄ selenium_trial/                    # Main project folder (pushes to GitHub)
‚îÇ   ‚îú‚îÄ‚îÄ extract_menu_hierarchy.py      # ‚≠ê MAIN CRAWLER - T24 menu extraction script
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies for selenium_trial
‚îÇ   ‚îú‚îÄ‚îÄ .env                           # Environment variables (credentials, config)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Output Files (Generated after extraction)
‚îÇ   ‚îú‚îÄ‚îÄ menu_hierarchy3.xlsx           # Primary output: Excel with 6,112 nodes
‚îÇ   ‚îú‚îÄ‚îÄ menu_hierarchy3.json           # Hierarchical JSON format
‚îÇ   ‚îú‚îÄ‚îÄ menu_hierarchy3.txt            # Breadcrumb text format
‚îÇ   ‚îú‚îÄ‚îÄ na_nodes_dom_paths.xlsx        # Analysis: DevTools paths for N/A nodes
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ archive_trials/                # Historical scripts & experiments
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md                  # Archive index
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CHECKPOINT_USAGE.md        # Checkpoint recovery documentation
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Early Crawlers (iterations 1-4)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crawler.py                 # Original basic crawler
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crawler_fast.py            # Speed optimizations attempt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crawler_iframe_aware.py    # iFrame handling iteration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crawler_with_stats.py      # Stats collection prototype
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Analysis Tools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze_crawler_output.py  # Output validation script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze_zero_patterns.py   # Investigate zero-element pages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze_na_patterns.py     # Analyze N/A XPath distribution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze_na_deep.py         # Deep analysis of N/A nodes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze_xpath_gap.py       # Compare extracted vs actual XPath
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Data Cleaning Tools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ check_dupes.py             # Detect duplicate entries
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_duplicates.py        # Remove duplicates from output
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ check_invalid_xpaths.py    # Validate XPath format
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Debugging & Rescreening
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ check_iframe_fields.py     # Debug iframe extraction issues
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ debug_iframe_fields.py     # iFrame debugging utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ debug_rescreen.py          # Rescreen specific items
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ debug_zero_elements.py     # Investigate zero-element issues
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rescreen_crawler.py        # Targeted rescreening script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rescreen_zero_elements.py  # Fix zero-element pages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rescreen_zero_xpath_pages.py # Fix zero-XPath pages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spot_check_zero.py         # Spot check zero-element nodes
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Testing & Validation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ compare_crawlers_test.py   # Compare different crawler versions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ compare_old_new.py         # Before/after comparison
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ read_excel.py              # Excel parsing utility
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_checkpoint.py         # Checkpoint recovery testing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validate_advanced.py       # Advanced validation suite
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validate_cleaned.py        # Validate cleaned data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validate_output.py         # Basic output validation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ verify_output.py           # Output verification script
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Output Files (Historical)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ menu_hierarchy[1-3].txt    # Historical breadcrumb outputs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ menu_hierarchy3.json       # Historical JSON backup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ uiMap_*.xlsx               # Various export iterations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page_stats_*.xlsx          # Page-level statistics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ element_count_summary.csv  # Per-page element counts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ zero_elements_*.csv        # Pages with zero clickable elements
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ zero_xpath_pages.xlsx      # Pages with zero XPath matches
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ items_to_rescreen.json     # Items flagged for rescreening
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Debug Artifacts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crawler_checkpoint.json    # Full backup checkpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crawler_fast_checkpoint.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page_screenshot_*.png      # Screenshots of problem pages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ debug_*.png                # Debug visual captures
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __pycache__/               # Python bytecode cache
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/                   # Python compiled modules
‚îÇ   ‚îî‚îÄ‚îÄ ~ (temp/lock files)            # Excel temp files (can be deleted)
‚îÇ
‚îú‚îÄ‚îÄ .git/                              # Git repository metadata
‚îú‚îÄ‚îÄ .venv/                             # Python virtual environment
‚îú‚îÄ‚îÄ .env                               # Root-level environment file
‚îú‚îÄ‚îÄ uiMap_out.xlsx                     # Root-level output sample
‚îî‚îÄ‚îÄ uiMap_out_report.html              # Root-level report sample
```

---

## Folder Descriptions

### üìÅ `.artifex/` - Documentation & Specifications
**Purpose**: Non-code documentation defining project standards, guidelines, and specifications

**Contents**:
- `GUIDELINES.md` - Development best practices and anti-patterns
- `SPECS.md` - Technical architecture and data schemas
- `FOLDER_STRUCTURE.md` - This file

**When to Use**: Reference this before coding, during code review, or when onboarding new developers

---

### üìÅ `selenium_trial/` - Main Project Folder
**Purpose**: Production-ready extraction code and outputs. This folder is pushed to GitHub.

**Key Files**:

#### `extract_menu_hierarchy.py` ‚≠ê MAIN ENTRY POINT
- Largest and most important file (~30KB)
- Complete T24 menu extraction pipeline
- Last modified: 2026-02-10 15:44
- Key functions:
  - `get_element_xpath(driver, element)` - JavaScript XPath extraction
  - `extract_node_info(li_element, ...)` - Element text/link extraction with nested search
  - `traverse_menu_tree(...)` - Recursive menu traversal
  - `build_full_paths(...)` - Breadcrumb construction
  - `export_to_excel/json/text()` - Multi-format export

**Recent Modification**: Changed element search from `./*` (direct children) to `.//*[self::a]` (all nested descendants) to capture deeply nested menu items. This fix resolved 1,556 N/A XPath entries.

#### `requirements.txt`
```
selenium>=4.10.0
openpyxl>=3.10.0
python-dotenv>=1.0.0
```

#### `.env` (credentials)
```
T24_USERNAME=<user>
T24_PASSWORD=<password>
T24_BASE_URL=http://<server>:<port>
HEADLESS_MODE=true
WAIT_TIMEOUT=30
```

#### Output Files (Auto-Generated)

| File | Format | Rows | Purpose |
|------|--------|------|---------|
| `menu_hierarchy3.xlsx` | Excel | 6,112 | Primary structured output with all columns |
| `menu_hierarchy3.json` | JSON | Nested | Hierarchical format for programmatic access |
| `menu_hierarchy3.txt` | Text | 6,112 | Breadcrumb format for human reading |
| `na_nodes_dom_paths.xlsx` | Excel | 1,556 | Analysis of nodes with N/A XPath (obsolete with fix) |

---

### üìÅ `archive_trials/` - Historical Scripts & Experiments
**Purpose**: Preserve prior development iterations and analysis tools for reference. These are NOT production code.

**Subfolder Organization**:

#### Early Crawlers
- `crawler.py` - Original iteration (slow, basic)
- `crawler_fast.py` - Optimization attempt
- `crawler_iframe_aware.py` - iFrame handling
- `crawler_with_stats.py` - Statistics collection prototype

**Why Archived**: Superseded by functionality integrated into `extract_menu_hierarchy.py`

#### Analysis Tools
- `analyze_* .py` - Scripts to investigate output patterns (NA nodes, zero elements, xpath gaps)

**Why Archived**: Used for one-time investigations; can be deleted if disk space needed

#### Data Cleaning Tools
- `check_*.py`, `clean_*.py` - Validation and data cleanup utilities

**When to Use**: If output contains duplicates or invalid entries, use these scripts to diagnose

#### Debugging & Rescreening
- `debug_*.py`, `rescreen_*.py` - Utilities to fix specific problem pages/nodes

**When to Use**: If extraction misses certain menu items or has N/A entries, use targeting rescreening tools

#### Testing & Validation
- `validate_*.py`, `test_*.py`, `compare_*.py` - Comparative testing between crawler versions

**When to Use**: After major code changes, run validation suite to ensure data quality

#### Historical Output Files
- `menu_hierarchy[1-3].*`, `uiMap_*.xlsx`, `page_stats_*.xlsx` - Outputs from prior runs

**Why Keep**: Track extraction history and compare before/after improvements

---

## File Naming Conventions

### Source Code
- `extract_menu_hierarchy.py` - Production extractor (MAIN)
- `crawler.py` / `crawler_*.py` - Crawler variants (archived)
- `analyze_*.py` - Analysis utilities (archived)
- `validate_*.py` - Validation tools (can be in root or archive)
- `debug_*.py` - Debugging utilities (archived)
- `rescreen_*.py` - Targeted rescreening (archived)

### Output Files (Generated)
- `menu_hierarchy3.xlsx` - Excel primary output (v3, latest)
- `menu_hierarchy3.json` - JSON backup (v3)
- `menu_hierarchy3.txt` - Text breadcrumb (v3)
- `na_nodes_dom_paths.xlsx` - Special analysis file
- `page_stats_*.xlsx` - Per-page statistics
- `zero_elements_*.csv` - Problem nodes list
- `*_checkpoint.json` - Resume points

### Configuration & Data
- `.env` - Environment variables (NEVER commit with real credentials)
- `requirements.txt` - Python dependencies
- `config.yaml` - Application configuration
- `*.png` - Debug screenshots
- `~$*.xlsx` - Excel lock files (temporary, ignore)

---

## GitHub Push Strategy

**What Gets Pushed**: Only `selenium_trial/` folder

**What Gets Excluded**:
- `.venv/` - Local Python environment
- `__pycache__/` - Compiled Python bytecode
- `.env` - Real credentials (use `.env.example` instead)
- `.git/` - Git metadata (auto-handled)
- Root-level output files (generated, not source)

**Git Configuration** (.gitignore):
```
.venv/
__pycache__/
.env
.env.local
*.pyc
~$*.xlsx
uiMap_out.xlsx
uiMap_out_report.html
```

**Commands to Push**:
```bash
cd xpath-crawler
git add selenium_trial/
git commit -m "Update XPath extraction with nested element fix"
git push origin main
```

---

## Folder Size Reference

| Folder | Size | Reason |
|--------|------|--------|
| `archive_trials/` | ~50 MB | Many Excel output files, screenshots, checkpoints |
| `selenium_trial/` (excl. archive) | ~10 MB | Main scripts, current outputs |
| `.venv/` | ~200 MB | Full Python environment |
| Total | ~260 MB | Can be reduced by deleting archive_trials |

---

## Development Workflow

### Initial Setup
```bash
cd xpath-crawler
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r selenium_trial/requirements.txt
cp selenium_trial/.env.example selenium_trial/.env
# Edit .env with real credentials
```

### Running Extraction
```bash
cd selenium_trial
python extract_menu_hierarchy.py
# Waits 3-8 hours... generates menu_hierarchy3.xlsx
```

### Data Validation
```bash
cd selenium_trial/archive_trials
python validate_output.py  # Check for duplicates, missing XPath
python analyze_na_patterns.py  # Investigate remaining N/A nodes
```

### Committing Changes
```bash
cd ..
git add selenium_trial/
git commit -m "Descriptive message of changes"
git push origin main
```

---

## Maintenance Instructions

### Cleaning Up After Extraction
1. **Delete temp files**: Remove `~$*.xlsx` lock files
2. **Backup checkpoint**: Save `*_checkpoint.json` in case of future failures
3. **Remove .pyc files**: `rm -r selenium_trial/__pycache__/`
4. **Archive old outputs**: Move `menu_hierarchy[1-2].*` to archive_trials/

### Archiving Trial Scripts
1. When a script is no longer needed, move it to `archive_trials/`
2. Update `archive_trials/README.md` with description
3. Keep only for reference/history‚Äîdon't clutter main folder

### Reducing Repo Size
- Archive very old output files: `.xlsx` files consume most space
- Remove unused `.png` screenshots after archiving
- Keep only recent 2-3 extraction outputs as references
- Current practical size: 10-15 MB for active code + recent outputs

---

## Troubleshooting by Folder

### Problem: `extract_menu_hierarchy.py` crashes midway
‚Üí Check `selenium_trial/*_checkpoint.json` exists
‚Üí Restart script‚Äîit will resume from checkpoint
‚Üí If checkpoint missing, script starts from beginning

### Problem: Output file shows N/A in XPath column
‚Üí This was **FIXED** by nested element detection
‚Üí Old archive files in `archive_trials/` may still show N/A
‚Üí Re-run `extract_menu_hierarchy.py` to get fresh output with XPath values

### Problem: Duplicate entries in output
‚Üí Run `archive_trials/clean_duplicates.py` on output file
‚Üí Or run `archive_trials/compare_crawlers_test.py` for analysis

### Problem: Zero clickable elements on certain pages
‚Üí Check `archive_trials/zero_elements_WITH_inputs.csv` for problem pages
‚Üí Use `archive_trials/rescreen_zero_elements.py` to investigate

### Problem: Can't find a menu item in output
‚Üí Try `archive_trials/spot_check_zero.py` to find missing items
‚Üí May need to adjust selectors if DOM structure changed

